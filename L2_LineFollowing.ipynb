{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L2_LineFollowing",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOlK03+aCeo/jpVIJWvmlRx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulodowd/EMATM0053_21_22/blob/main/L2_LineFollowing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zROCR6hT1fea"
      },
      "source": [
        "# Labsheet 2: Line Following\n",
        "\n",
        "This labsheet has exercises for you to develop two different approaches to using the ground sensor - the simulated sensor we will use to get the simulated robot to follow a line of the arena floor.\n",
        "\n",
        "- Bang-Bang Controller\n",
        "- Weighted-Measurement \n",
        "- Proportional Controller\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MRCOE6y91l7"
      },
      "source": [
        "<hr><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqTnOH1n11tx"
      },
      "source": [
        "## Ground Sensor (Line Sensor)\n",
        "\n",
        "The Webots simulation of the e-puck has a model of a <a href=\"http://www.e-puck.org/index.php?option=com_content&view=article&id=17&Itemid=18\">real ground sensor</a> developed for the <a href=\"http://www.e-puck.org/\">real e-puck</a>. \n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/Webots_GroundSensor.png?raw=true\">\n",
        "</p>\n",
        "\n",
        "The ground sensor works by transmitting a beam of infra-red light, and then measuring the intensity of the reflection.  Different surfaces, such as black and white colour, reflect the light with different intensity.  The intensity of reflected light is a continuous signal, and would be measured by an **`analog-to-digital converter`** - a specialist piece of <a href=\"https://en.wikipedia.org/wiki/Analog-to-digital_converter\">electronic hardware</a>.  \n",
        "\n",
        "The sensing elements (**`transducers`**) that measure the reflectance would be considered to be an **`analog sensor`**.  The real ground sensor (not simulation) is queried by the microcontroller using the I2C protocol ( 'i'-squared-C, <a href=\"https://en.wikipedia.org/wiki/I%C2%B2C\">\"inter-integrated circuit\"</a>), which is a method of transferring information digitally.  Therefore, from a system interface perspective, the device as a whole is considered **`digital`**.\n",
        "\n",
        "<p align=\"center\">\n",
        "<br>\n",
        "<img src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/Webots_GSReflectance.png?raw=true\">\n",
        "<br>\n",
        "</p>\n",
        "\n",
        "\n",
        "The <a href=\"http://www.e-puck.org/index.php?option=com_phocadownload&view=file&id=18:ground-sensor-user-manual&Itemid=38\">user manual</a> tells us that the inside-distance between the two sensors on the outside edges is 12mm.  Therefore, using some **`closed-loop feedback control`**, we can keep a line of 12mm or less between the two external sensors.  From this, we can build some line following behaviour. \n",
        "\n",
        "We can quickly sketch out a plan to produce line following behaviour from the knowledge we have so far.  In the illustration below, there are 3 examples of the robot in different scenarios with respect to the line:\n",
        "\n",
        "<p align=\"center\">\n",
        "<br>\n",
        "<img src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/Webots_LineFeedback.png?raw=true\">\n",
        "<br>\n",
        "</p>\n",
        "\n",
        "We can quickly draft the following logic:\n",
        "\n",
        "- In **`scenario (A)`**, the central sensor is activated by the black line, and so the robot can continue with it's normal direction of travel.  This is the ideal scenario for line following.  Here, the ideal is to keep the black line between the inactivated outside sensors.  Note that, whilst we (humans) can observe the robot _will_ leave the line, presently the robot does not have this information.\n",
        "\n",
        "- In **`scenario (B)`**, the robot has veered off the line to the **`left`**, causing the right-most sensor to become activated.  To bring the line back under the centre sensor, the appropriate feedback is to **`turn right`**.\n",
        "\n",
        "- In **`scenario (C)`**, the robot has veered off the line to the **`right`**, causing the left-most sensor to become activated.  To bring the line back under the centre sensor, the appropriate feedback is to **`turn left`**.\n",
        "\n",
        "In essence, we can write a controller to keep the black line between the left and right sensors.  Note, there are other ways to utilise this sensor to follow the line.  \n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq6yTCzX-E6S"
      },
      "source": [
        "## Exercise 1: Ground Sensor (30 minutes)\n",
        "\n",
        "Feel free to discuss these questions with your peers:\n",
        "\n",
        "1. Is the ground sensor an **`active`** or **`passive`** sensor?  What informs your answer?\n",
        "\n",
        "2. Using the logic provided above, what would you expect to observe your robot do if it started off the line (not on the line)?\n",
        "  \n",
        "3. What motion would you expect to observe in the robot motion if the feedback-signal was inverted?\n",
        "  - is there a case where line following can be achieved with an inverted feedback signal?\n",
        "\n",
        "4. An exceptional case not caught with the above logic would be if all three ground-sensors were activated simultaneously.  This can happen even though the black line is less than 12mm wide.  Under what condition might this occur?\n",
        "\n",
        "5. What would be appropriate feedback responses for exceptional (non-defined) cases of the sensor activation?\n",
        "\n",
        "6. If your robot was to calculate a performance score within `loop()` for line-following (a **`metric`**) as it operated:\n",
        "  - what **`proprioceptive`** information could be used?\n",
        "  - what **`exteroceptive`** information could be used?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lU1okwB9yj9"
      },
      "source": [
        "<hr><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9njgPAAK66tu"
      },
      "source": [
        "## Bang-Bang Controller\n",
        "\n",
        "The most intuitive form of controller we can write for line following is a **`bang-bang`** controller.  A bang-bang controller has this name because the control decisions tend to be sudden and dramatic (bang!), moving between discrete states (bang-bang).  \n",
        "\n",
        "A consequence of this can be that there is no gradation in behaviour, or the behaviour looks jerky.  Usually, a bang-bang controller is not a good controller to use, but it is a good place to start if you have not programmed before.  It will help to develop a sense of how `conditional statements` can be used to branch code, to change the flow of your program, and so change the observable robot behaviour.  \n",
        "\n",
        "A bang-bang controller is built using a series of **`if()`** statements.  We will use the `if()` statement to check if a condition is true, and if so, effect a specific behaviour.  \n",
        "\n",
        "We can interpret the above section and write some **`pseudo-code`**:\n",
        "\n",
        "```c\n",
        "if( left_sensor == active_threshold ) {\n",
        "  // Turn right.\n",
        "\n",
        "} else if( right_sensor == active_threshold ) {\n",
        "  // Turn left\n",
        "\n",
        "} else if ( center_sensor == active_threshold ) {\n",
        "  // Move forwards\n",
        "\n",
        "} else {\n",
        "  // Are there other conditions?\n",
        "  // What is an appropriate response?\n",
        "\n",
        "}\n",
        "```\n",
        "\n",
        "To implement this code in Webots, you will need to use the built-in functions to both read the ground sensors and to set the appropriate motor velocities.  \n",
        "\n",
        "From `Labsheet 1`, you should be experienced with commanding the robot to drive forwards/backwards and to rotate.  To read the ground sensor, the template example (`labsheet_x.c`) provides the following code to report the values to the console:\n",
        "\n",
        "```c\n",
        "  // Get latest ground sensor readings\n",
        "  gs_value[0] = wb_distance_sensor_get_value(gs[0]);\n",
        "  gs_value[1] = wb_distance_sensor_get_value(gs[1]);  \n",
        "  gs_value[2] = wb_distance_sensor_get_value(gs[2]);\n",
        "\n",
        "   // Report ground sensor values\n",
        "  printf(\"Ground sensor values: \\n\");\n",
        "  printf(\" 0: %d\\n\", gs_value[0] );\n",
        "  printf(\" 1: %d\\n\", gs_value[1] );\n",
        "  printf(\" 2: %d\\n\", gs_value[2] );\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4QcOytBV9aP"
      },
      "source": [
        "## Exercise 2: Bang-Bang Controller (4 hours)\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/Webots_StartStraight.png?raw=true\">\n",
        "</p>\n",
        "\n",
        "1. Is a bang-bang controller **`open-loop`** or **`closed-loop`** control?  What is the difference?\n",
        "\n",
        "2. Before you write code to command your robot to move, use the graphical user interface to position your robot ground sensor over the black line and record the ground sensor values.\n",
        "  - Of the three sensors indexed `0,1,2`, which is left, centre and right?\n",
        "  - What are typical values over a white surface?\n",
        "  - What are typical values over a black surface?\n",
        "  - How much **`variance`** is there in these readings?\n",
        "  - What would be a good value to use as a **`threshold`** for the condition of your if() statements?\n",
        "\n",
        "\n",
        "3. Implement the discussed bang-bang controller logic to achieve line-following:\n",
        "  - it is easiest to start the simulation with your robot already placed on a `straight` section of the line.  Check that it can follow a straight line first.\n",
        "  - it is recommended you start with `slow` or `low` motor velocities.\n",
        "  - check that your `feedback signal` turns your robot in the appropriate direction.\n",
        "  - **help**: remember your conditional statement can use: \n",
        "    - `<`  less than\n",
        "    - `<=` less than or equal to\n",
        "    - `==` equal to\n",
        "    - `>=` greater than or equal to\n",
        "    - `>`  greater than\n",
        "    - `!=` not equal to\n",
        "  - **help**: what is the functional difference between the two code examples immediately below?\n",
        "  \n",
        "  ```c\n",
        "  // Example 1\n",
        "  if( ) {\n",
        "\n",
        "  } \n",
        "  if( ) {\n",
        "\n",
        "  }\n",
        "\n",
        "  // Example 2\n",
        "  if( ) {\n",
        "\n",
        "  } else if( ) {\n",
        "\n",
        "  } \n",
        "  ```\n",
        "\n",
        "4. Does your robot conduct `turn` and `move fowards` operations seperately?  \n",
        "  - Can these be integrated so that the robot does not stop moving forwards?\n",
        "  - How is performance effected?\n",
        "  - Write down a set of **`task requirements`** where fast forward speed might be desirable for the robotic system.\n",
        "  - What is the quickest forward speed you can utilise and still achieve reliable line-following?\n",
        "  - For a selected foward speed, which elements of the line following map can be completed, and which cannot?  Decide upon a set of consistent labels for the elements of the line following map.\n",
        "  - Decide a discrete list of forward speed intervals to capture the **`failure modes`** you observe.  Are the different forward speeds clearly seperable in their line following performance?\n",
        "\n",
        "5. Start your robot off the line, and allow it to travel forward to join and follow the line.  Currently, what is the most extreme <a href=\"https://en.wikipedia.org/wiki/Angle_of_incidence_(optics)\">angle of incidence</a> where your controller can still successfully begin line following?\n",
        "  - if you were to create a results table of different angles when joining the line, how could you quantify the reliability of the controller?\n",
        "\n",
        "6. What information about the line does the robot have when no sensors are activated?\n",
        "  - When might this circumstance occur?\n",
        "  - What would be an appropriate response in this condition?\n",
        "  - What other information is available to the robot that might be useful?\n",
        "  \n",
        "7. Write a function to simply confirm if the robot is on a black line.  The function should report a `true` or `false` value when called.\n",
        "  - is there a reason to discriminate between which of the sensors is active?  Explain your reasoning, adjust the function if necessary.\n",
        "  \n",
        "8. Write a specific function to operate the robot when it is **`initialised`** (\"powered on\" for the first time) off the line, so that it can successfully join a line at 90 degrees (orthogonal) to the forward direction of travel. \n",
        "  - Aligning your robot to 45 degrees (approx. `0.785` radians) in the `start box` on the provided arena floor will achieve this starting condition.\n",
        "  - Decide what your robot should do when it meets an orthogonal line, or how to bias your robot behaviour away from this circumstance.\n",
        "  - Create a **`global variable`** called **`STATE`** to indicate if the robot is operating to `join line` or `follow line`.  Which state should the robot initialise into when powered on? \n",
        "  - Use the `setup()` function in the template to set initial variable values, such as `STATE`.\n",
        "  - Use an `if()` statement to switch the behaviour between `join line` or `follow line` depending on the global variable `STATE`.\n",
        "  - Use the prior line detection function (`true` / `false`) to transition your robot between the **`join line`** behaviour, and the **`follow line`** behaviour.  \n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh1oodxgE7xk"
      },
      "source": [
        "<hr><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aRGTk_UJnMu"
      },
      "source": [
        "## Calibration ( I think this section is not relevant for simulation )\n",
        "\n",
        "In the previous exercises, we have set arbitrary `threshold` values to make logical decisions.  We likely found the threshold values through a `trial-and-error` experimental process.  \n",
        "\n",
        "You may have noticed that within Webots, the ground sensor is simulated to have typical values in the range 200 - 800, and they fluctate within a small amount.  A good question to ask would be \"why does the range not begin from 0?\"  \n",
        "\n",
        "The answer to this question is device and hardware specific.  A general answer is that the sensor is effected by environmental conditions.  First of all, external ambient light is likely to exist and to interfere with the measurement of reflected light.  Second, the ground sensor is `active` - transmitting light to be measured - creating a `bias` or `offset` in readings when reflectance occurs.  \n",
        "\n",
        "Another important consideration is that the `task environment` of the robot can change within the duration of the robot's operation.  For a robot measuring the reflectance of infra-red light from a surface, ambient light (such as sun-light) is a likely cause of **`signal interference`**.  If the ambient light conditions change, we can expect that the level and range of our sensor readings would change also.  If we have used fixed `threshold` values, we can expect these to become incorrect if the `task environment` changes significantly.\n",
        "- additional ambient light might cause an offset or bias in readings (to raise or lower all measurements)\n",
        "- additional light may reduce the overall **`sensitivity`**, reducing the effective **`range`** of measurements.  \n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/Webots_GS_Interference.png?raw=true\">\n",
        "<br>\n",
        "</p>\n",
        "\n",
        "With the simulated ground sensor, the bias is not effected by the ambient light level.  We can consider the bias in the sensor reading to be a **`systematic error`** - this means a persistent and generally consistent error in value inherent to the operation of the device.  We can also observe some small high-frequency fluctuations in the sensor readings, which constitute **`random error`**.  As a general rule:\n",
        "- `calibration` is used to mitigate `systematic error`.\n",
        "- `filtering` is used to mitigate `random error`.\n",
        "\n",
        "\n",
        "\n",
        "<pre>\n",
        "<!--\n",
        "In fact, it may be desirable that a sensor is engineered to operate within a range larger than expected, so that the sensor should not **`saturate`** (become over energised).  For example, if the sensor saturates at a maximum value, any higher measurements from the environment cannot be represented.  In audio recordings, we can hear this as clipping - when the amplitude of sound waves exceeds the capability of the recording equipment.\n",
        "-->\n",
        "</pre>\n",
        "\n",
        "  \n",
        "We can use a **`calibration`** routine to identify the characteristics of the operation of a device.   \n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img width=\"75%\" src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/Webots_CalibrationPlot.png?raw=true\">\n",
        "</p>\n",
        "\n",
        "  On a more complicated sensor, we could use calibration to also identify error and remove this from sensor reading.  \n",
        "\n",
        "\n",
        "\n",
        "We can use calibration to identify the actual range of the sensor and any bias, and then use this information to normalise the readings to exist within a percentage range.\n",
        "\n",
        "\n",
        "\n",
        "Depending on the electronic engineering, some devices can be **`active-low`**, whilst others can be **`active-high`**.  For a reflectance sensor, **`active-low`** would mean that greater intensity of reflection would `lower` the measured value.  For `active-high`, greater intensity of reflection would `raise` the measured value.  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4lK0byARwas"
      },
      "source": [
        "## Exercise 3: Calibration\n",
        "\n",
        "1. Is the ground sensor within Webots `active-low` or `active-high`?\n",
        "\n",
        "2. When the ground sensor measurement is given by a microcontroller in the range of approximately 200-800, what units would this be in?\n",
        "\n",
        "3. On a microcontroller, what hardware specification would determine the maximum possible range of values from an analogue sensor measurement?  The <a href=\"https://www.pololu.com/category/285/original-3pi-plus-32u4-robot\">Pololu 3Pi+</a> robot we use on this unit of study has an Atmega 32u4 microcontroller (<a href=\"https://ww1.microchip.com/downloads/en/DeviceDoc/Atmel-7766-8-bit-AVR-ATmega16U4-32U4_Datasheet.pdf\">datasheet</a>, <a href=\"https://www.microchip.com/en-us/product/ATmega32U4\">product page</a>).\n",
        "\n",
        "4. Write a calibration routine to:\n",
        "  - collect `n` number of samples of reflectance from a black line.\n",
        "  - collect `n` number of samples of reflectance from the white surface.\n",
        "  - determine the mean (average) reflectance for the white surface and the black line respectively.\n",
        "  - **Help:** In this `task environment`, we know that the black line and the white surface represent the conditions for the minimum and maximum possible values of sensor readings.  Therefore, you can program a behaviour to have your robot rotate on the spot, so that the sensor passes over both black and white surfaces, and collect the sensor readings at the extreme values.  You can start your robot so that it's axles are inline with the back edge of the starting box on the line following map.  \n",
        "\n",
        "  <p align=\"center\">\n",
        "<img src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/Webots_StartOnEdge.png?raw=true\">\n",
        "</p>\n",
        "\n",
        "  - **Help:** When you later use a calibration routine to solve the complete line following challenge, rotating your robot on the spot might be detrimental to final performance.  Why would this be?  What other behaviour could be used to collect the minimum and maximum sensor values?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAG4VDTbJrMm"
      },
      "source": [
        "<hr><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMDZBvtm0Oss"
      },
      "source": [
        "## Weighted-Measurement\n",
        "\n",
        "So far when we have been reading and using the ground sensor, we have made `logical` decisions based on the reported value.  We have chosen through a process of `trial-and-error` experimentation a fixed `threshold` value for our conditional statements.  However, it is more desirable if the robot can determine an appropriate response **`autonomously`** without hard-coded (fixed) threshold values.  Weighted-Measurement in an example technique that will allow us to achieve this aim.  \n",
        "\n",
        "The sensor reports numerical values so it is possible to perform some calculations.  The ground sensor reports a range of values representative of a continuous scale of reflectivity.  This range of values provides much more information from the state of reality (simulation, in this case).  \n",
        "\n",
        "We can consider that if a sensor elements is only half across a black line, it will not report a \"perfect\" black value:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img width=\"50%\" src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/Webots_GSReflectanceHalf.png?raw=true\">\n",
        "</p>\n",
        "\n",
        "Furthermore, the black line on the arena floor is sized to fit between the two outer sensor elements, which are spaced 12mm apart.  In which case, there is a continuous range of possible conditions where 1 or 2 or 3 sensing elements are either fully covered, partially covered or not covered at all.  \n",
        "\n",
        "These properties of the sensor and environment interaction can be used to our advantage.  The numerical readings from the 3 sensor elements can be combined to approximate where the line may be under the whole ground sensor.  By analogy, we can think of this as taking the sensor readings and placing them on to a traditional weigh-scale.  The \"weigh-scale\" then represents a useful transformation of the data: \n",
        "\n",
        "- The direction the scale tips indicates the relative position of the line under the robot, or **which way** to turn.\n",
        "- The magnitude of tip of the scale indicates how far the line has moved from the centre of the robot, or **how much** to turn.\n",
        "- when we have both `magnitude` and `direction`, we can think of this as a **`vector`**.  \n",
        "\n",
        "<p align=\"center\">\n",
        "<img width=\"50%\" src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/Webots_GS_Weighted.png?raw=true\">\n",
        "</p>\n",
        "\n",
        "We want to \"weigh\" each of the sensor readings against each other.  We could first calculate the weight of each sensing element with respect to the theoretical maximum range of the sensor ([ 0 : 1023 ]).  However, we already know that the sensor does not utilise the full range, with readings typically falling between a value of 200 and 800.  The below illustrates a typical **`sensor response`** of one sensing element:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img width=\"66%\" src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/Webots_CalibrationPlot.png?raw=true\">\n",
        "</p>\n",
        "\n",
        "Alternatively, the weight of each sensor reading can be calculated with respect to the total activation (`summation`) of the ground sensor in that instance.  Because we expect the value of summation to vary between the uses of our ground sensor, it is useful to `normalise` the sensor values relative to the total activation, creating an output between [ 0.0 : 1.0 ].  This consistent output value range will make subsequent calculations or conditional statements more generalisable.   \n",
        "\n",
        "<p align=\"center\">\n",
        "<img width=\"75%\" src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/Webots_WeightedMeasurement.png?raw=true\">\n",
        "</p>\n",
        "\n",
        "With the normalised sensor values, we have the `proportional weight` of each ground sensor element (in proportion against the total activation).  Because the central sensor is between the left and right sensors, we will utilise the central sensor weight to contribute to both the left and right weights in our final weighted-measurement calculation:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/Webots_WeightedLineEq.png?raw=true\">\n",
        "</p>\n",
        "\n",
        "where `gs0` is the left-most sensor, `gs1` is the central sensor, and `gs2` is the right-most sensor.  In `eq(1)` and `eq(2)`, we add half of the central sensor value, implemented by the `gain` of `0.5` in  both cases.  In `eq(3)`, we subtract the weighted-right (`w_right`) from the weighted-left (`w_left`), which provides our `error signal`, `e_line`.  \n",
        "\n",
        "There are other methods to achieve a weighted line measurement (e.g., <a href=\"http://www.micromouseonline.com/2011/04/15/simpler-line-follower-sensors/\">here</a>), and you are free to determine your own methods.  \n",
        "\n",
        "We expect the `error signal` to be a value ranging between [ -1.0 : +1.0 ].  We can then use this information to decide which way to turn, and by how much.  Because the `error signal` is a value between [ -1.0 : +1.0 ], we can also use this to `proportionally control` a turning velocity.  If we hard-code (fix) a known maximum turning velocity, then values between [ -1.0 : +1.0 ] will scale the value appropriately.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4_PYfC9fm_7"
      },
      "source": [
        "## Exercise 3: Weighted-Line Measurement\n",
        "\n",
        "1.  Implement the weighted-line sensing as discussed above. There are still further considerations to make:\n",
        "  - is your ground sensor `active low` or `active high`?   How could you invert the value of a sensor reading?  Is this necessary?\n",
        "  - does the `sign` (-/+) of the error signal intuitively indicate the direction your robot should turn?  Which direction of rotation should -/+ be?\n",
        "  - how can you invert the sign of the error signal?\n",
        "  - **Decompose the problem**: first of all, test your robot without any forward velocity.  Is your robot able to re-centre itself over the line?  Check by manually placing your robot in different circumstances.\n",
        "  - **help**: What would you expect your robot to do when it is not on a line?  Why is this?\n",
        "  - **help**: The following code extract may help you to get started:\n",
        "\n",
        "```c\n",
        "\n",
        "void loop() {\n",
        "\n",
        "  // Get the line error\n",
        "  float e_line;\n",
        "  e_line = getLineError();\n",
        "\n",
        "  // Determine a proportional rotation speed\n",
        "  float turn_velocity;\n",
        "  turn_velocity = ????;  // What is a sensible maximum speed?\n",
        "  turn_velocty = turn_velocity * e_line;\n",
        "\n",
        "  // Set motor values.\n",
        "  // What does \"0 -\" and \"0 +\" achieve here?\n",
        "  wb_motor_set_velocity(right_motor, 0 - turn_velocity);\n",
        "  wb_motor_set_velocity(left_motor, 0 + turn_velocity);\n",
        "  \n",
        "}\n",
        "\n",
        "// A function to return an error signal representative\n",
        "// of the line placement under the ground sensor.\n",
        "float getLineError() {\n",
        "  float e_line;\n",
        "\n",
        "  // Read ground sensor, store result\n",
        "\n",
        "  // Sum ground sensor activation\n",
        "\n",
        "  // Normalise individual sensor readings\n",
        "\n",
        "  // Calculated error signal\n",
        "  e_line = ????;\n",
        "\n",
        "  // Return result\n",
        "  return e_line;\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "2. Explore the gain value for the central sensing element (described with value `0.5` above:\n",
        "  - What is the effect of alternative gain values?\n",
        "  - What other formulation could be used to utilise the weighted 3 sensing elements in combination?\n",
        "\n",
        "3. Modify your solution to (1) to implement a forward velocity with your robot.\n",
        "  - How fast can your robot move forwards and still reliably follow the line?\n",
        "  - Which parts of the line following map present the greatest challenge to your solution?\n",
        "  \n",
        "\n",
        "4. Implement a mechanism to vary the foward velocity of your robot independently of the turning velocity:\n",
        "  - Aim for high forward velocity on straight-line segments, and low forward velocity on sharp corners.\n",
        "  - How can your robot measure it's own line following performance?\n",
        "  - How can you convert this line performance measure into another `proportional control` mechanism for forward velocity?\n"
      ]
    }
  ]
}