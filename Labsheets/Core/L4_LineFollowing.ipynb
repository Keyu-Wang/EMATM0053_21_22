{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L4_LineFollowing",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNxtVhuAJ2DSQiCTGvJNuJ5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulodowd/EMATM0053_21_22/blob/main/Labsheets/Core/L4_LineFollowing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjtThWpXj72W"
      },
      "source": [
        "# Labsheet 4: Line Following\n",
        "\n",
        "In previous labsheets we have:\n",
        "\n",
        "- Familiarised with the Arduino IDE, Sketch files, uploading to the 3Pi+, and utilising the Serial interface\n",
        "- Implemented basic operation of the 3Pi+ motors, and encapsulated this within safe and confident function(s).\n",
        "- Implemented basic read functionality of the 3 central IR Line sensors facing the work surface, and encapsulated this within function(s).\n",
        "- Explored the use of `millis()` to approximate task-scheduling on the 3Pi+.\n",
        "\n",
        "In this labsheet we will:\n",
        "- Utilise prior work with the line sensors and motors to implement line following behaviour on the 3Pi+.\n",
        "- Develop a **bang-bang** controller, using `logic` to control the robot.\n",
        "- Implement a calibration routine for the ground sensor.  \n",
        "- Develop a **weighted-measurement**, using a `proportional controller` to control the robot.\n",
        "\n",
        "\n",
        "If you have prior experience with programming, you may wish to skip the section for the bang-bang controller.  The bang-bang controller is generally inefficient, but provides a good introduction to the concepts of providing feedback control to the 3Pi+.  It is still recommended to read through the bang-bang controller section regardless. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zmlP5ctkme1"
      },
      "source": [
        "<hr><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg53dyHHko38"
      },
      "source": [
        "# Line Following, in Overview\n",
        "\n",
        "In this labsheet, we will continue to work with just the 3 central ground sensors, labelled `DN2`, `DN3` and `DN4` on the 3PI+ and in the documentation.  The black vinyl tape provided with your 3Pi+ fits between the two outer sensors, `DN2` and `DN4`.  \n",
        "\n",
        "<p align=\"center\">\n",
        "<br>\n",
        "<img src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/3PI_BottomAnnotated.png?raw=true\">\n",
        "<br>\n",
        "</p>\n",
        "\n",
        "We can quickly sketch out a plan to produce line following behaviour from the knowledge we have so far.  In the illustration below, there are 3 examples of the robot in different scenarios with respect to the line:\n",
        "\n",
        "<p align=\"center\">\n",
        "<br>\n",
        "<img src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/Webots_LineFeedback.png?raw=true\">\n",
        "<br>\n",
        "</p>\n",
        "\n",
        "We can quickly draft the following logic:\n",
        "\n",
        "- In **`scenario (A)`**, the central sensor is activated by the black line, and so the robot can continue with it's normal direction of travel.  This is the ideal scenario for line following.  Here, the ideal is to keep the black line between the inactivated outside sensors.  Note that, whilst we (humans) can observe the robot _will_ leave the line, presently the robot does not have this information.\n",
        "\n",
        "- In **`scenario (B)`**, the robot has veered off the line to the **`left`**, causing the right-most sensor to become activated.  To bring the line back under the centre sensor, the appropriate feedback is to **`turn right`**.\n",
        "\n",
        "- In **`scenario (C)`**, the robot has veered off the line to the **`right`**, causing the left-most sensor to become activated.  To bring the line back under the centre sensor, the appropriate feedback is to **`turn left`**.\n",
        "\n",
        "In essence, we can write a controller to keep the black line between the left and right sensors.  Note, there are other ways to utilise this sensor to follow the line.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FPo6H4Rl9Ca"
      },
      "source": [
        "## Exercise 1: Ground Sensors (15 minutes)\n",
        "\n",
        "Feel free to discuss these questions with your peers:\n",
        "\n",
        "1. Given your prior experience of the line-sensors, what do we expect the time-values of the 3 ground sensors to be on white and black surfaces?\n",
        "  - in the above example, we have considered the central sensor to be \"active\" on the line, but we could also define this in inverted terms.  \n",
        "  \n",
        "2. What motion would you expect to observe in the robot motion if the feedback-signal was inverted?\n",
        "  - is there a case where line following can be achieved with an inverted feedback signal?\n",
        "\n",
        "3. An exceptional case not caught with the above logic would be if all three ground-sensors were detecting back simultaneously.  This can happen even though the black line is narrower than the gap between the two outer-most sensors.  Under what condition might this occur?\n",
        "\n",
        "4. What would be appropriate feedback responses for other exceptional (non-defined) cases of the sensor activation?\n",
        "\n",
        "5. If your robot was to calculate a performance score within `loop()` for line-following (a **`metric`**) as it operated:\n",
        "  - what **`proprioceptive`** information could be used?\n",
        "  - what **`exteroceptive`** information could be used?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JBMX6iPoata"
      },
      "source": [
        "<hr><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZztyE_cXoZ1j"
      },
      "source": [
        "# Bang-Bang Controller\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/3PI_PhotoShortLine.jpg?raw=true\">\n",
        "</p>\n",
        "\n",
        "The most intuitive form of controller we can write for line following is a **`bang-bang`** controller.  A bang-bang controller has this name because the control decisions tend to be sudden and dramatic (bang!), moving between discrete states (bang<->bang).  \n",
        "\n",
        "A consequence of this can be that there is no gradation in behaviour, or the behaviour looks jerky.  Usually, a bang-bang controller is not a good controller to use, but it is a good place to start if you have not programmed before.  It will help to develop a sense of how `conditional statements` can be used to branch code, to change the flow of your program, and so change the observable robot behaviour.  \n",
        "\n",
        "A bang-bang controller is built using a series of **`if()`** statements.  We will use the `if()` statement to check if a condition is true, and if so, effect a specific behaviour.  \n",
        "\n",
        "We can interpret the above section and write some **`pseudo-code`**:\n",
        "\n",
        "```c\n",
        "if( left_sensor == active_threshold ) {\n",
        "  // Turn right.\n",
        "\n",
        "} else if( right_sensor == active_threshold ) {\n",
        "  // Turn left\n",
        "\n",
        "} else if ( center_sensor == active_threshold ) {\n",
        "  // Move forwards\n",
        "\n",
        "} else {\n",
        "  // Are there other conditions?\n",
        "  // What is an appropriate response?\n",
        "\n",
        "}\n",
        "```\n",
        "\n",
        "From Labsheet 3, you will have implemented a sensor reading function.  From Labsheet 2, you will have implemented some motor command functions.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y6KNPmDpfP3"
      },
      "source": [
        "## Exercise 2: Implement a Bang-Bang Controller\n",
        "\n",
        "1. Is a bang-bang controller **`open-loop`** or **`closed-loop`** control?  What is the difference?\n",
        "\n",
        "2. Using your prior experience taking readings from the ground sensors, decide appropriate thresholds for when each sensor can be considered `active` or `inactive`:\n",
        "  - decide whether large time measurements are active or inactive.\n",
        "  - which sensor states will be used to build the bang-bang control logic discussed above?\n",
        "  - are these threshold values the same for `DN2`, `DN3` and `DN4`?  Remember that you can inspect the values of your sensors using `Serial.print()` and `Serial.println()`.\n",
        "  - it is recommended to use `#define` statements to set these values at the top of your program.\n",
        "\n",
        "3. Implement the discussed bang-bang controller logic within your `loop()`:\n",
        "  - use the black vinyl tape to create a short black line on your work surface.\n",
        "  - it is recommended you start with `slow` or `low` motor velocities.\n",
        "  - to begin with, do not implement forward motion.  Instead, work only with turning on the spot (rotation).\n",
        "  - check that your `feedback signal` turns your robot in the appropriate directions with respect to sensor activation.\n",
        "  - investigate what happens when your line is placed into difficult places with respect to the line - are any of these a problem you need to address?\n",
        "  - once you are confident rotation is working properly, implement forward motion.\n",
        "  - **help**: remember your conditional statement can use: \n",
        "    - `<`  less than\n",
        "    - `<=` less than or equal to\n",
        "    - `==` equal to\n",
        "    - `>=` greater than or equal to\n",
        "    - `>`  greater than\n",
        "    - `!=` not equal to\n",
        "  - **help**: what is the functional difference between the two code examples immediately below?\n",
        "  \n",
        "```c\n",
        "  // Example 1\n",
        "  if( ) {\n",
        "\n",
        "  } \n",
        "  if( ) {\n",
        "\n",
        "  }\n",
        "\n",
        "  // Example 2\n",
        "  if( ) {\n",
        "\n",
        "  } else if( ) {\n",
        "\n",
        "  } \n",
        "```\n",
        "\n",
        "4. Does your robot conduct `turn` and `move fowards` operations seperately?  \n",
        "  - Can these be integrated so that the robot does not stop moving forwards?\n",
        "    - it may be useful to use your bang-bang logic to set the value of left and right `pwm` or `power` variables, which are then used to command the motors once after the logic.\n",
        "  - How is performance effected with turning and moving forwards combined?\n",
        "  - Moving slowly might increase the general reliability of the line following behaviour.  As a thought experiment, what other hypothetical **`task requirements`** would make fast forward speed desirable for the robotic system?\n",
        "  - What is the quickest forward speed you can utilise and still achieve reliable line-following?\n",
        "  - If you have not done so already, experiment with more challenging line shapes, such as corners and curves.  \n",
        "\n",
        "5. What information about the line does the robot have when no sensors detect the black surface?\n",
        "  - When might this circumstance occur?  Consider the line map provided for the line following challenge.\n",
        "  - What would be an appropriate response in this condition?\n",
        "  - What other information is available to the robot that might be useful?\n",
        "\n",
        "6. Write a function to simply confirm if the robot is on a black line.  The function should report a `true` or `false` value when called.\n",
        "  - is there a reason to discriminate between which of the sensors is on a black surface, or can it be any of the 3?  Explain your reasoning, adjust the function if necessary.\n",
        "\n",
        "7. Use the above function to allow your robot to start off the line and drive forwards until it meets the line.  Once it is on the line, it should activate the bang-bang controller.\n",
        "  - Consider using a `global` variable and an `if()` statement to switch the robot between these two behaviours. \n",
        "\n",
        "8. Start your robot off the line, and allow it to travel forward to join and follow the line.  Currently, what is the most extreme <a href=\"https://en.wikipedia.org/wiki/Angle_of_incidence_(optics)\">angle of incidence</a> where your controller can still successfully begin line following?\n",
        "  - if you were to create a results table of different angles when joining the line, how could you quantify the reliability of the controller?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8ixoGl_wPQP"
      },
      "source": [
        "<hr><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK3lQju7wZpH"
      },
      "source": [
        "# Weighted-Measurement\n",
        "\n",
        "The bang-bang controller has the disadvantage that the combinations of logic can become increasingly large, complex and unwiedly.  Another severe limitation of the bang-bang controller is that it uses `hard-coded` (fixed) parameter values to determine behaviour.  If there is a substantial change in the environment (such as a change to ambient infra-red light, e.g., the sun coming out), these values might become invalid.  \n",
        "\n",
        "<p align=\"center\">\n",
        "<img width=\"75%\" src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/3PI_DN_Interference.png?raw=true\">\n",
        "</p>\n",
        "\n",
        "It is more desirable if the robot can determine an appropriate response **`autonomously`** without hard-coded (fixed) threshold values.  Weighted-Measurement in an example technique that will allow us to achieve this aim.  \n",
        "\n",
        "Our sensor reading process returns numerical values so it is possible to perform some calculations.  The ground sensor reports a range of values of time representative of a continuous scale of reflectivity.  This range of values provides much more information from the state of reality.\n",
        "\n",
        "We can consider that if a sensor elements is only half across a black line, it will not report a \"perfect\" black value:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img width=\"50%\" src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/3PI_DN_ReflectanceHalf.png?raw=true\">\n",
        "</p>\n",
        "\n",
        "Furthermore, the black tape size fits between the two outer sensor elements, `DN2` and `DN4`.  In which case, there is a continuous range of possible conditions where 1 or 2 or 3 sensing elements are either fully covered, partially covered or not covered at all.  \n",
        "\n",
        "These properties of the sensor and environment interaction can be used to our advantage.  The numerical readings from the 3 sensor elements can be combined to approximate where the line may be under the whole ground sensor.  By analogy, we can think of this as taking the sensor readings and placing them on to a traditional weigh-scale.  The \"weigh-scale\" then represents a useful transformation of the data: \n",
        "\n",
        "- The direction the scale tips indicates the relative position of the line under the robot, or **which way** to turn.\n",
        "- The magnitude of tip of the scale indicates how far the line has moved from the centre of the robot, or **how much** to turn.\n",
        "- when we have both `magnitude` and `direction`, we can think of this as a **`vector`**.  \n",
        "\n",
        "<p align=\"center\">\n",
        "<img width=\"50%\" src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/3PI_DN_Weighted.png?raw=true\">\n",
        "</p>\n",
        "\n",
        "We want to \"weigh\" each of the sensor readings against each other.  We could first calculate the weight of each sensing element with respect to the theoretical maximum range of the sensor measurements.  However, we already know that the range of measurements can vary depending on the environment conditions (e.g., interference from ambient light).  The below illustrates a typical **`sensor response`** of one sensing element:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img width=\"66%\" src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/3PI_CalibrationPlot.png?raw=true\">\n",
        "</p>\n",
        "\n",
        "Alternatively, the weight of each sensor reading can be calculated with respect to the total activation (`summation`) of the ground sensor in that instance.  Because we expect the value of summation to vary between the uses of our ground sensor, it is useful to `normalise` the sensor values relative to the total activation, creating an output between [ 0.0 : 1.0 ].  This consistent output value range will make subsequent calculations or conditional statements more generalisable.   \n",
        "\n",
        "<p align=\"center\">\n",
        "<img width=\"75%\" src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/3PI_WeightedMeasurement.png?raw=true\">\n",
        "</p>\n",
        "\n",
        "With the normalised sensor values, we have the `proportional weight` of each ground sensor element (in proportion against the total activation).  Because the central sensor is between the left and right sensors, we will utilise the central sensor weight to contribute to both the left and right weights in our final weighted-measurement calculation:\n",
        "\n",
        "<p align=\"center\">\n",
        "$w_{left} = L_{left} + ( L_{centre}  * 0.5 )$\n",
        "</p>\n",
        "<p align=\"center\">\n",
        "$w_{right} = L_{right} + ( L_{centre}  * 0.5 )$\n",
        "</p>\n",
        "<p align=\"center\">\n",
        "$e_{line} = w_{left} - w_{right}$\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "where $L$ are ground sensors. For both $w_{left}$ and $w_{right}$ we add half of the central sensor value, implemented by the `gain` of `0.5` in  both cases.  We can think of this as \"sharing the influence\", or halfing the influence.  For $e_{line}$, we subtract the weighted-right ($w_{right}$) from the weighted-left ($w_{left}$), which provides our `error signal`, $e_{line}$.  \n",
        "\n",
        "There are other methods to achieve a weighted line measurement (e.g., <a href=\"http://www.micromouseonline.com/2011/04/15/simpler-line-follower-sensors/\">here</a>), and you are free to determine your own methods.  \n",
        "\n",
        "We expect the `error signal` to be a value ranging between [ -1.0 : +1.0 ].  We can then use this information to decide which way to turn, and by how much.  Because the `error signal` is a value between [ -1.0 : +1.0 ], we can also use this to `proportionally control` a turning velocity.  If we hard-code (fix) a known maximum turning velocity, then values between [ -1.0 : +1.0 ] will scale the value appropriately.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}